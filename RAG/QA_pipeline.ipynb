{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f982ef93",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import torch\n",
    "import numpy as np\n",
    "from transformers import AutoTokenizer, AutoModelForTokenClassification, pipeline\n",
    "from transformers import AutoModelForQuestionAnswering, AutoTokenizer as QATokenizer\n",
    "from neo4j import GraphDatabase\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import torch\n",
    "\n",
    "# ======================\n",
    "#      Configuration\n",
    "# ======================\n",
    "NER_MODEL_NAME = \"Helios9/BioMed_NER\"\n",
    "QA_MODEL_NAME = \"deepset/bert-base-cased-squad2\"\n",
    "NEO4J_URI = \"bolt://localhost:7687\"\n",
    "NEO4J_USER = \"neo4j\"\n",
    "NEO4J_PASSWORD = \"valmik_neo4j\"\n",
    "EMBEDDING_MODEL_PATH = \"node2vec_model.pt\"\n",
    "MAX_CONTEXT_LENGTH = 512\n",
    "\n",
    "# ======================\n",
    "#  Custom Word2Vec Class \n",
    "# ======================\n",
    "class CustomWord2Vec:\n",
    "    def __init__(self, vocab, W1, word2idx, idx2word):\n",
    "        self.vocab = vocab\n",
    "        self.W1 = W1.cpu()\n",
    "        self.word2idx = word2idx\n",
    "        self.idx2word = idx2word\n",
    "    \n",
    "    def wv(self, word):\n",
    "        return self.W1[self.word2idx[word]].cpu().numpy()\n",
    "\n",
    "# ======================\n",
    "#      NER Component\n",
    "# ======================\n",
    "class BiomedicalNER:\n",
    "    def __init__(self):\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(NER_MODEL_NAME)\n",
    "        self.model = AutoModelForTokenClassification.from_pretrained(NER_MODEL_NAME)\n",
    "        self.pipeline = pipeline(\"ner\", model=self.model, tokenizer=self.tokenizer, aggregation_strategy=\"simple\")\n",
    "    \n",
    "    def extract_entities(self, text):\n",
    "        ner_results = self.pipeline(text)\n",
    "        entities = []\n",
    "        seen = set()\n",
    "\n",
    "        for entity in ner_results:\n",
    "            if entity[\"entity_group\"] == \"O\":\n",
    "                continue\n",
    "            text_segment = text[entity[\"start\"]:entity[\"end\"]]\n",
    "            text_segment_clean = text_segment.strip().lower()\n",
    "            if text_segment_clean not in seen:\n",
    "                seen.add(text_segment_clean)\n",
    "                entities.append((text_segment_clean, entity[\"entity_group\"]))\n",
    "        \n",
    "        # Merge Detailed_description + Sign_symptom\n",
    "        merged_entities = []\n",
    "        i = 0\n",
    "        while i < len(entities):\n",
    "            if i < len(entities) - 1:\n",
    "                current_ent, next_ent = entities[i], entities[i+1]\n",
    "                if current_ent[1] == \"Detailed_description\" and next_ent[1] == \"Sign_symptom\":\n",
    "                    merged_text = f\"{current_ent[0]} {next_ent[0]}\"\n",
    "                    merged_entities.append((merged_text, \"Sign_symptom\"))\n",
    "                    i += 2\n",
    "                    continue\n",
    "            merged_entities.append(entities[i])\n",
    "            i += 1\n",
    "\n",
    "        return [ent[0] for ent in merged_entities]\n",
    "\n",
    "# ======================\n",
    "#  Embedding & Similarity\n",
    "# ======================\n",
    "class NodeEmbeddings:\n",
    "    def __init__(self, model_path):\n",
    "        with torch.serialization.safe_globals([CustomWord2Vec]):\n",
    "            self.model = torch.load(\n",
    "                model_path,\n",
    "                map_location=torch.device('cpu'),\n",
    "                weights_only=True\n",
    "            )\n",
    "        self.vocab = self.model.vocab\n",
    "        self.word2idx = self.model.word2idx\n",
    "        self.embeddings = self.model.W1.cpu().numpy()\n",
    "        \n",
    "    def get_embedding(self, text):\n",
    "        words = text.lower().split()\n",
    "        valid_embs = []\n",
    "        for word in words:\n",
    "            if word in self.word2idx:\n",
    "                valid_embs.append(self.embeddings[self.word2idx[word]])\n",
    "        return np.mean(valid_embs, axis=0) if valid_embs else None\n",
    "\n",
    "    def top_similar_nodes(self, embedding, top_k=5):\n",
    "        if embedding is None:\n",
    "            return []\n",
    "        similarities = cosine_similarity([embedding], self.embeddings)[0]\n",
    "        top_indices = np.argsort(similarities)[-top_k:][::-1]\n",
    "        return [(self.vocab[i], similarities[i]) for i in top_indices]\n",
    "\n",
    "# ======================\n",
    "#    Neo4j Connector\n",
    "# ======================\n",
    "class Neo4jConnector:\n",
    "    def __init__(self, uri, user, password):\n",
    "        self.driver = GraphDatabase.driver(uri, auth=(user, password))\n",
    "        \n",
    "    def get_relationships(self, node_name):\n",
    "        with self.driver.session() as session:\n",
    "            result = session.run(\"\"\"\n",
    "                MATCH (a)-[r]->(b)\n",
    "                WHERE toLower(a.name) = toLower($name)\n",
    "                RETURN a.name, type(r) as rel_type, b.name\n",
    "                UNION\n",
    "                MATCH (a)<-[r]-(b)\n",
    "                WHERE toLower(a.name) = toLower($name)\n",
    "                RETURN a.name, type(r) as rel_type, b.name\n",
    "            \"\"\", name=node_name)\n",
    "            return [(record[\"a.name\"], record[\"rel_type\"], record[\"b.name\"]) for record in result]\n",
    "\n",
    "# ======================\n",
    "#       QA Component\n",
    "# ======================\n",
    "class BiomedicalQA:\n",
    "    def __init__(self):\n",
    "        self.qa_tokenizer = QATokenizer.from_pretrained(QA_MODEL_NAME)\n",
    "        self.qa_model = AutoModelForQuestionAnswering.from_pretrained(QA_MODEL_NAME)\n",
    "        \n",
    "    def answer_question(self, context, question):\n",
    "        if not context.strip():\n",
    "            return \"No relevant information found in the knowledge base\"\n",
    "            \n",
    "        try:\n",
    "            inputs = self.qa_tokenizer(\n",
    "                question,\n",
    "                context,\n",
    "                add_special_tokens=True,\n",
    "                max_length=MAX_CONTEXT_LENGTH,\n",
    "                truncation=\"only_second\",\n",
    "                padding=\"max_length\",\n",
    "                return_tensors=\"pt\"\n",
    "            )\n",
    "            \n",
    "            outputs = self.qa_model(**inputs)\n",
    "            start_logits = outputs.start_logits.detach().numpy().flatten()\n",
    "            end_logits = outputs.end_logits.detach().numpy().flatten()\n",
    "            \n",
    "            # Handle context boundaries\n",
    "            token_type_ids = inputs.token_type_ids.numpy().flatten()\n",
    "            context_indices = np.where(token_type_ids == 1)[0]\n",
    "            \n",
    "            if len(context_indices) == 0:\n",
    "                return \"No relevant context available for answering\"\n",
    "                \n",
    "            context_start = context_indices[0]\n",
    "            context_end = context_indices[-1]\n",
    "            \n",
    "            # Mask non-context positions\n",
    "            start_logits[:context_start] = -np.inf\n",
    "            end_logits[:context_start] = -np.inf\n",
    "            start_logits[context_end+1:] = -np.inf\n",
    "            end_logits[context_end+1:] = -np.inf\n",
    "            \n",
    "            start_idx = np.argmax(start_logits)\n",
    "            end_idx = np.argmax(end_logits)\n",
    "            \n",
    "            if end_idx < start_idx or start_idx == 0:\n",
    "                return \"No clear answer found in the knowledge base\"\n",
    "                \n",
    "            answer_tokens = inputs[\"input_ids\"][0][start_idx:end_idx+1]\n",
    "            answer = self.qa_tokenizer.decode(answer_tokens, skip_special_tokens=True).strip()\n",
    "            \n",
    "            return answer if answer else \"No specific answer found\"\n",
    "            \n",
    "        except Exception as e:\n",
    "            return f\"Error processing answer: {str(e)}\"\n",
    "\n",
    "# ======================\n",
    "#    Main Pipeline\n",
    "# ======================\n",
    "def main_pipeline(user_query):\n",
    "    ner = BiomedicalNER()\n",
    "    entities = ner.extract_entities(user_query)\n",
    "    print(f\"Extracted entities: {entities}\")\n",
    "    \n",
    "    node_embeddings = NodeEmbeddings(EMBEDDING_MODEL_PATH)\n",
    "    neo4j_conn = Neo4jConnector(NEO4J_URI, NEO4J_USER, NEO4J_PASSWORD)\n",
    "    \n",
    "    context_chunks = []\n",
    "    for entity in entities:\n",
    "        emb = node_embeddings.get_embedding(entity)\n",
    "        if emb is None:\n",
    "            continue\n",
    "            \n",
    "        similar_nodes = node_embeddings.top_similar_nodes(emb)\n",
    "        for node_name, _ in similar_nodes:\n",
    "            relationships = neo4j_conn.get_relationships(node_name)\n",
    "            for src, rel, tgt in relationships:\n",
    "                rel_clean = rel.replace('_', ' ').lower()\n",
    "                context_chunks.append(f\"{src} {rel_clean} {tgt}\".lower())\n",
    "    \n",
    "    # Create focused context with unique sentences\n",
    "    unique_chunks = list(set(context_chunks))\n",
    "    if not unique_chunks:\n",
    "        return \"No relevant medical information found in the knowledge base\"\n",
    "        \n",
    "    context = \" \".join(sorted(unique_chunks, key=lambda x: len(x)))[:3000]\n",
    "    print(f\"\\nGenerated context ({len(context)} chars): {context[:500]}...\")\n",
    "    \n",
    "    qa = BiomedicalQA()\n",
    "    return qa.answer_question(context, user_query)\n",
    "\n",
    "# ======================\n",
    "#       Execution\n",
    "# ======================\n",
    "if __name__ == \"__main__\":\n",
    "    user_query = input(\"Enter your medical question: \")\n",
    "    answer = main_pipeline(user_query)\n",
    "    print(f\"\\nAnswer: {answer}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0efd775",
   "metadata": {},
   "source": [
    "AP code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e046a0b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from transformers import AutoTokenizer, AutoModelForTokenClassification, pipeline\n",
    "from transformers import GPT2LMHeadModel, GPT2Tokenizer\n",
    "from neo4j import GraphDatabase\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "# ======================\n",
    "#      Configuration\n",
    "# ======================\n",
    "NER_MODEL_NAME = \"Helios9/BioMed_NER\"\n",
    "QA_MODEL_NAME = \"gpt2\"\n",
    "NEO4J_URI = \"bolt://localhost:7687\"\n",
    "NEO4J_USER = \"neo4j\"\n",
    "NEO4J_PASSWORD = \"valmik_neo4j\"\n",
    "EMBEDDING_MODEL_PATH = \"node2vec_model.pt\"\n",
    "MAX_CONTEXT_LENGTH = 512\n",
    "MAX_GENERATION_LENGTH = 150\n",
    "\n",
    "# ======================\n",
    "#  Custom Word2Vec Class \n",
    "# ======================\n",
    "class CustomWord2Vec:\n",
    "    def __init__(self, vocab, W1, word2idx, idx2word):\n",
    "        self.vocab = vocab\n",
    "        self.W1 = W1.cpu()\n",
    "        self.word2idx = word2idx\n",
    "        self.idx2word = idx2word\n",
    "    \n",
    "    def wv(self, word):\n",
    "        return self.W1[self.word2idx[word]].cpu().numpy()\n",
    "\n",
    "# ======================\n",
    "#      NER Component\n",
    "# ======================\n",
    "class BiomedicalNER:\n",
    "    def __init__(self):\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(NER_MODEL_NAME)\n",
    "        self.model = AutoModelForTokenClassification.from_pretrained(NER_MODEL_NAME)\n",
    "        self.pipeline = pipeline(\"ner\", model=self.model, tokenizer=self.tokenizer, aggregation_strategy=\"simple\")\n",
    "    \n",
    "    def extract_entities(self, text):\n",
    "        ner_results = self.pipeline(text)\n",
    "        entities = []\n",
    "        seen = set()\n",
    "\n",
    "        for entity in ner_results:\n",
    "            if entity[\"entity_group\"] == \"O\":\n",
    "                continue\n",
    "            text_segment = text[entity[\"start\"]:entity[\"end\"]]\n",
    "            text_segment_clean = text_segment.strip().lower()\n",
    "            if text_segment_clean not in seen:\n",
    "                seen.add(text_segment_clean)\n",
    "                entities.append((text_segment_clean, entity[\"entity_group\"]))\n",
    "        \n",
    "        # Merge Detailed_description + Sign_symptom\n",
    "        merged_entities = []\n",
    "        i = 0\n",
    "        while i < len(entities):\n",
    "            if i < len(entities) - 1:\n",
    "                current_ent, next_ent = entities[i], entities[i+1]\n",
    "                if current_ent[1] == \"Detailed_description\" and next_ent[1] == \"Sign_symptom\":\n",
    "                    merged_text = f\"{current_ent[0]} {next_ent[0]}\"\n",
    "                    merged_entities.append((merged_text, \"Sign_symptom\"))\n",
    "                    i += 2\n",
    "                    continue\n",
    "            merged_entities.append(entities[i])\n",
    "            i += 1\n",
    "\n",
    "        return [ent[0] for ent in merged_entities]\n",
    "\n",
    "# ======================\n",
    "#  Embedding & Similarity\n",
    "# ======================\n",
    "class NodeEmbeddings:\n",
    "    def __init__(self, model_path):\n",
    "        with torch.serialization.safe_globals([CustomWord2Vec]):\n",
    "            self.model = torch.load(\n",
    "                model_path,\n",
    "                map_location=torch.device('cpu'),\n",
    "                weights_only=True\n",
    "            )\n",
    "        self.vocab = self.model.vocab\n",
    "        self.word2idx = self.model.word2idx\n",
    "        self.embeddings = self.model.W1.cpu().numpy()\n",
    "        \n",
    "    def get_embedding(self, text):\n",
    "        words = text.lower().split()\n",
    "        valid_embs = []\n",
    "        for word in words:\n",
    "            if word in self.word2idx:\n",
    "                valid_embs.append(self.embeddings[self.word2idx[word]])\n",
    "        return np.mean(valid_embs, axis=0) if valid_embs else None\n",
    "\n",
    "    def top_similar_nodes(self, embedding, top_k=5):\n",
    "        if embedding is None:\n",
    "            return []\n",
    "        similarities = cosine_similarity([embedding], self.embeddings)[0]\n",
    "        top_indices = np.argsort(similarities)[-top_k:][::-1]\n",
    "        return [(self.vocab[i], similarities[i]) for i in top_indices]\n",
    "\n",
    "# ======================\n",
    "#    Neo4j Connector\n",
    "# ======================\n",
    "class Neo4jConnector:\n",
    "    def __init__(self, uri, user, password):\n",
    "        self.driver = GraphDatabase.driver(uri, auth=(user, password))\n",
    "        \n",
    "    def get_relationships(self, node_name):\n",
    "        with self.driver.session() as session:\n",
    "            result = session.run(\"\"\"\n",
    "                MATCH (a)-[r]->(b)\n",
    "                WHERE toLower(a.name) = toLower($name)\n",
    "                RETURN a.name, type(r) as rel_type, b.name\n",
    "                UNION\n",
    "                MATCH (a)<-[r]-(b)\n",
    "                WHERE toLower(a.name) = toLower($name)\n",
    "                RETURN a.name, type(r) as rel_type, b.name\n",
    "            \"\"\", name=node_name)\n",
    "            return [(record[\"a.name\"], record[\"rel_type\"], record[\"b.name\"]) for record in result]\n",
    "\n",
    "# ======================\n",
    "#     GPT2 QA Component\n",
    "# ======================\n",
    "class BiomedicalQA:\n",
    "    def __init__(self):\n",
    "        self.tokenizer = GPT2Tokenizer.from_pretrained(QA_MODEL_NAME)\n",
    "        self.model = GPT2LMHeadModel.from_pretrained(QA_MODEL_NAME)\n",
    "        self.model.eval()\n",
    "    \n",
    "    def answer_question(self, context, question):\n",
    "        if not context.strip():\n",
    "            return \"No relevant information found in the knowledge base\"\n",
    "\n",
    "        prompt = f\"Context: {context}\\nQuestion: {question}\\nAnswer:\"\n",
    "        inputs = self.tokenizer(prompt, return_tensors=\"pt\", truncation=True, max_length=MAX_CONTEXT_LENGTH)\n",
    "        with torch.no_grad():\n",
    "            outputs = self.model.generate(\n",
    "                **inputs,\n",
    "                max_length=inputs['input_ids'].shape[1] + MAX_GENERATION_LENGTH,\n",
    "                pad_token_id=self.tokenizer.eos_token_id,\n",
    "                do_sample=True,\n",
    "                top_k=50,\n",
    "                top_p=0.95\n",
    "            )\n",
    "        answer = self.tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "        return answer.split(\"Answer:\")[-1].strip()\n",
    "\n",
    "# ======================\n",
    "#    Main Pipeline\n",
    "# ======================\n",
    "def main_pipeline(user_query):\n",
    "    ner = BiomedicalNER()\n",
    "    entities = ner.extract_entities(user_query)\n",
    "    print(f\"Extracted entities: {entities}\")\n",
    "    \n",
    "    node_embeddings = NodeEmbeddings(EMBEDDING_MODEL_PATH)\n",
    "    neo4j_conn = Neo4jConnector(NEO4J_URI, NEO4J_USER, NEO4J_PASSWORD)\n",
    "    \n",
    "    context_chunks = []\n",
    "    for entity in entities:\n",
    "        emb = node_embeddings.get_embedding(entity)\n",
    "        if emb is None:\n",
    "            continue\n",
    "            \n",
    "        similar_nodes = node_embeddings.top_similar_nodes(emb)\n",
    "        for node_name, _ in similar_nodes:\n",
    "            relationships = neo4j_conn.get_relationships(node_name)\n",
    "            for src, rel, tgt in relationships:\n",
    "                rel_clean = rel.replace('_', ' ').lower()\n",
    "                context_chunks.append(f\"{src} {rel_clean} {tgt}\".lower())\n",
    "    \n",
    "    unique_chunks = list(set(context_chunks))\n",
    "    if not unique_chunks:\n",
    "        return \"No relevant medical information found in the knowledge base\"\n",
    "        \n",
    "    context = \" \".join(sorted(unique_chunks, key=lambda x: len(x)))\n",
    "    print(f\"\\nGenerated context ({len(context)} chars): {context}...\")\n",
    "    \n",
    "    qa = BiomedicalQA()\n",
    "    return qa.answer_question(context, user_query)\n",
    "\n",
    "# ======================\n",
    "#       Execution\n",
    "# ======================\n",
    "if __name__ == \"__main__\":\n",
    "    user_query = input(\"Enter your medical question: \")\n",
    "    answer = main_pipeline(user_query)\n",
    "    print(f\"\\nAnswer: {answer}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlpenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
