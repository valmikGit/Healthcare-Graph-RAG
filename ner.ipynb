{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install necessary libraries\n",
    "!pip install transformers\n",
    "!pip install torch\n",
    "\n",
    "# Import libraries\n",
    "from transformers import AutoTokenizer, AutoModelForTokenClassification\n",
    "from transformers import pipeline\n",
    "\n",
    "# Load the model and tokenizer\n",
    "model_name = \"d4data/biomedical-ner-all\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForTokenClassification.from_pretrained(model_name)\n",
    "\n",
    "# Create a NER pipeline\n",
    "nlp = pipeline(\"ner\", model=model, tokenizer=tokenizer, aggregation_strategy=\"simple\")\n",
    "\n",
    "# Sample medical text\n",
    "text = \"\"\"The patient was diagnosed with diabetes and prescribed metformin. \n",
    "They reported frequent urination and fatigue, and later developed hypertension.\"\"\"\n",
    "\n",
    "# Run the NER pipeline\n",
    "ner_results = nlp(text)\n",
    "\n",
    "# Display the extracted named entities\n",
    "print(\"Named Entities Found:\\n\")\n",
    "for entity in ner_results:\n",
    "    print(f\"{entity['word']}: {entity['entity_group']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install transformers torch -q\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModelForTokenClassification\n",
    "from transformers import pipeline\n",
    "\n",
    "# Load model and tokenizer\n",
    "model_name = \"d4data/biomedical-ner-all\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForTokenClassification.from_pretrained(model_name)\n",
    "\n",
    "# NER pipeline with basic config\n",
    "ner_pipeline = pipeline(\"ner\", model=model, tokenizer=tokenizer, grouped_entities=False)\n",
    "\n",
    "# Input clinical text\n",
    "text = \"\"\"The patient was diagnosed with diabetes and prescribed metformin.\n",
    "They reported frequent urination and fatigue, and later developed hypertension.\"\"\"\n",
    "\n",
    "# Run the pipeline\n",
    "ner_results = ner_pipeline(text)\n",
    "\n",
    "# Custom post-processing to merge subwords and same-entity tokens\n",
    "entities = []\n",
    "current_entity = \"\"\n",
    "current_label = \"\"\n",
    "for item in ner_results:\n",
    "    word = item[\"word\"]\n",
    "    label = item[\"entity\"]\n",
    "\n",
    "    # Clean up subwords\n",
    "    if word.startswith(\"##\"):\n",
    "        current_entity += word[2:]\n",
    "    elif label == current_label:\n",
    "        current_entity += \" \" + word\n",
    "    else:\n",
    "        if current_entity:\n",
    "            entities.append((current_entity.strip(), current_label.split(\"_\")[-1]))\n",
    "        current_entity = word\n",
    "        current_label = label\n",
    "\n",
    "# Append last entity\n",
    "if current_entity:\n",
    "    entities.append((current_entity.strip(), current_label.split(\"_\")[-1]))\n",
    "\n",
    "# Display results\n",
    "print(\"Named Entities Found:\\n\")\n",
    "for word, label in entities:\n",
    "    print(f\"{word}: {label}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install transformers torch -q\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModelForTokenClassification\n",
    "from transformers import pipeline\n",
    "\n",
    "# Load model and tokenizer\n",
    "model_name = \"d4data/biomedical-ner-all\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForTokenClassification.from_pretrained(model_name)\n",
    "\n",
    "# NER pipeline\n",
    "ner_pipeline = pipeline(\"ner\", model=model, tokenizer=tokenizer, grouped_entities=False)\n",
    "\n",
    "# Input clinical text\n",
    "text = \"\"\"The patient was diagnosed with diabetes and prescribed metformin.\n",
    "They reported frequent urination and fatigue, and later developed hypertension.\"\"\"\n",
    "\n",
    "# Run NER pipeline\n",
    "ner_results = ner_pipeline(text)\n",
    "\n",
    "# Improved post-processing to handle B- and I- tags and merge subwords\n",
    "entities = []\n",
    "current_entity = \"\"\n",
    "current_label = \"\"\n",
    "last_index = -1\n",
    "\n",
    "for item in ner_results:\n",
    "    word = item[\"word\"]\n",
    "    label = item[\"entity\"]\n",
    "    index = item[\"index\"]\n",
    "\n",
    "    label_type = label.split(\"-\")[-1]\n",
    "    prefix = label.split(\"-\")[0] if \"-\" in label else \"\"\n",
    "\n",
    "    # Merge subword tokens\n",
    "    if word.startswith(\"##\"):\n",
    "        current_entity += word[2:]\n",
    "        continue\n",
    "\n",
    "    # Start of a new entity\n",
    "    if prefix == \"B\" or label != current_label or index != last_index + 1:\n",
    "        if current_entity:\n",
    "            entities.append((current_entity.strip(), current_label.split(\"-\")[-1]))\n",
    "        current_entity = word\n",
    "        current_label = label\n",
    "    else:  # continuation of an entity (I-tag or repeated label)\n",
    "        current_entity += \" \" + word\n",
    "\n",
    "    last_index = index\n",
    "\n",
    "# Append the last entity if exists\n",
    "if current_entity:\n",
    "    entities.append((current_entity.strip(), current_label.split(\"-\")[-1]))\n",
    "\n",
    "# Display results\n",
    "print(\"Named Entities Found:\\n\")\n",
    "for word, label in entities:\n",
    "    print(f\"{word}: {label}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install transformers torch -q\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModelForTokenClassification\n",
    "from transformers import pipeline\n",
    "\n",
    "# Load model and tokenizer\n",
    "model_name = \"d4data/biomedical-ner-all\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForTokenClassification.from_pretrained(model_name)\n",
    "\n",
    "# NER pipeline\n",
    "ner_pipeline = pipeline(\"ner\", model=model, tokenizer=tokenizer, grouped_entities=False)\n",
    "\n",
    "# Input clinical text\n",
    "text = \"\"\"Diarrhea, also spelled diarrhoea, is the condition of having at least three loose, liquid, or watery bowel movements each day. It often lasts for a few days and can result in dehydration due to fluid loss. Signs of dehydration often begin with loss of the normal stretchiness of the skin and irritable behaviour. This can progress to decreased urination, loss of skin color, a fast heart rate, and a decrease in responsiveness as it becomes more severe. Loose but non-watery stools in babies who are exclusively breastfed, however, are normal.\"\"\"\n",
    "\n",
    "# Run NER pipeline\n",
    "ner_results = ner_pipeline(text)\n",
    "\n",
    "# Improved post-processing using character positions\n",
    "entities = []\n",
    "current_entity = \"\"\n",
    "current_label = \"\"\n",
    "current_start = None\n",
    "current_end = None\n",
    "\n",
    "for item in ner_results:\n",
    "    word = item[\"word\"]\n",
    "    label = item[\"entity\"]\n",
    "    start = item[\"start\"]\n",
    "    end = item[\"end\"]\n",
    "    \n",
    "    # Split entity label\n",
    "    if \"-\" in label:\n",
    "        prefix, entity_type = label.split(\"-\")\n",
    "    else:\n",
    "        prefix, entity_type = \"\", label\n",
    "\n",
    "    # Skip non-entity tokens\n",
    "    if entity_type == \"O\":\n",
    "        if current_entity:\n",
    "            entities.append((current_entity, current_label, current_start, current_end))\n",
    "            current_entity = \"\"\n",
    "            current_label = \"\"\n",
    "        continue\n",
    "\n",
    "    # Handle subwords and entity continuity\n",
    "    if current_entity:\n",
    "        # Check if current token continues the previous entity\n",
    "        if (start == current_end) and (entity_type == current_label):\n",
    "            if word.startswith(\"##\"):\n",
    "                current_entity += word[2:]\n",
    "            else:\n",
    "                current_entity += \" \" + word\n",
    "            current_end = end\n",
    "        else:\n",
    "            # Finalize current entity and start new\n",
    "            entities.append((current_entity, current_label, current_start, current_end))\n",
    "            current_entity = word if not word.startswith(\"##\") else word[2:]\n",
    "            current_label = entity_type\n",
    "            current_start = start\n",
    "            current_end = end\n",
    "    else:\n",
    "        # Start new entity\n",
    "        current_entity = word if not word.startswith(\"##\") else word[2:]\n",
    "        current_label = entity_type\n",
    "        current_start = start\n",
    "        current_end = end\n",
    "\n",
    "# Add the last entity\n",
    "if current_entity:\n",
    "    entities.append((current_entity, current_label, current_start, current_end))\n",
    "\n",
    "# Merge overlapping/adjacent entities and clean up\n",
    "final_entities = []\n",
    "for entity in entities:\n",
    "    text_segment = text[entity[2]:entity[3]]\n",
    "    \n",
    "    # Handle minor position mismatches\n",
    "    if entity[0].lower() != text_segment.lower():\n",
    "        final_entity = text_segment\n",
    "    else:\n",
    "        final_entity = entity[0]\n",
    "    \n",
    "    final_entities.append((final_entity, entity[1]))\n",
    "\n",
    "# Merge Detailed_description followed by Sign_symptom\n",
    "merged_entities = []\n",
    "i = 0\n",
    "while i < len(final_entities):\n",
    "    if i < len(final_entities) - 1:\n",
    "        current_ent, next_ent = final_entities[i], final_entities[i+1]\n",
    "        # Check for Detailed_description followed by Sign_symptom\n",
    "        if current_ent[1] == \"Detailed_description\" and next_ent[1] == \"Sign_symptom\":\n",
    "            merged_text = f\"{current_ent[0]} {next_ent[0]}\"\n",
    "            merged_entities.append((merged_text, \"Sign_symptom\"))\n",
    "            i += 2  # Skip next element\n",
    "            continue\n",
    "    merged_entities.append(final_entities[i])\n",
    "    i += 1\n",
    "\n",
    "# Remove duplicates and filter non-entities\n",
    "seen = set()\n",
    "unique_entities = []\n",
    "for ent in merged_entities:\n",
    "    if ent[1] != \"O\" and ent[0] not in seen:\n",
    "        seen.add(ent[0])\n",
    "        unique_entities.append(ent)\n",
    "\n",
    "# Display results\n",
    "print(\"Named Entities Found:\\n\")\n",
    "for word, label in unique_entities:\n",
    "    print(f\"{word}: {label}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlpenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
